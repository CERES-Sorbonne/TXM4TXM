{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "\n",
    "\n",
    "def getReplique(p):\n",
    "    if p.find(\"p\"):\n",
    "        texte = p.find(\"p\").text\n",
    "    elif p.find(\"l\"):\n",
    "        texte = p.find(\"l\").text\n",
    "    else:\n",
    "        texte = p.text\n",
    "\n",
    "    return re.sub(r\"(\\s)\\1+\", r\"\\g<1>\", texte).strip()\n",
    "\n",
    "def elaguer(texte):\n",
    "    return re.sub(r\"\\s+\", \" \", texte).strip()\n",
    "\n",
    "original_folder = Path(\"textes Codif\")\n",
    "output_folder = Path(\"forTXM\")\n",
    "output_folder.mkdir(exist_ok=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T13:38:42.600975991Z",
     "start_time": "2023-08-30T13:38:42.172948344Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "metas = []\n",
    "for piece in original_folder.glob(\"*.xml\"):\n",
    "    with piece.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"xml\")\n",
    "\n",
    "    TEI = soup.find(\"TEI\")\n",
    "    teiHeader = TEI.find(\"teiHeader\")\n",
    "    text = TEI.find(\"text\")\n",
    "\n",
    "    meta = {\n",
    "        \"Titre\": teiHeader.find(\"title\").text,\n",
    "        \"Edition\": teiHeader.find(\"edition\").text if teiHeader.find(\"edition\") else \"N/A\",\n",
    "        \"Publication\": teiHeader.find(\"publicationStmt\").find(\"publisher\").text if teiHeader.find(\n",
    "            \"publicationStmt\").find(\"publisher\") else \"N/A\",\n",
    "        \"Date\": teiHeader.find(\"publicationStmt\").find(\"date\").attrs[\"when\"] if teiHeader.find(\"publicationStmt\").find(\n",
    "            \"date\") and \"when\" in teiHeader.find(\"publicationStmt\").find(\"date\").attrs else \"N/A\",\n",
    "        \"Source\": elaguer(teiHeader.find(\"sourceDesc\").text),\n",
    "    }\n",
    "    meta = {k: v.strip() for k, v in meta.items()}\n",
    "\n",
    "    personnages = text.find(\"castList\").findAll(\"castItem\")\n",
    "    personnages = [\n",
    "        {\"Id\": p.attrs[\"xml:id\"], \"Display\": elaguer(p.text)} if \"xml:id\" in p.attrs else {\"Id\": \"N/A\", \"Display\": p.text} for p\n",
    "        in personnages]\n",
    "    meta[\"Personnages\"] = personnages\n",
    "\n",
    "    responsables = teiHeader.findAll(\"respStmt\")\n",
    "    responsables = [{\"Name\": r.find(\"name\").text, \"Role\": r.find(\"resp\").text} for r in responsables]\n",
    "    meta[\"Responsables\"] = responsables\n",
    "\n",
    "    text = text.find(\"body\")\n",
    "\n",
    "    actes = [\n",
    "        {\n",
    "            \"Numéro\": a.attrs[\"n\"],\n",
    "            \"Titre\": a.find(\"head\").text,\n",
    "            \"Stage\": a.find(\"stage\").text,\n",
    "            \"Scènes\": [\n",
    "                {\n",
    "                    \"Numéro\": s.attrs[\"n\"],\n",
    "                    \"Titre\": s.find(\"head\").text,\n",
    "                    \"Scène\": s.find(\"stage\").text if s.find(\"stage\") else \"N/A\",\n",
    "                    \"Paroles\": [\n",
    "                        {\n",
    "                            \"Personnage\": {\n",
    "                                \"Id\": p.attrs[\"who\"] if \"who\" in p.attrs else \"N/A\",\n",
    "                                \"Display\": p.find(\"speaker\").text if p.find(\"speaker\") else \"N/A\",\n",
    "                            },\n",
    "                            \"Réplique\": getReplique(p),\n",
    "                        }\n",
    "                        for p in s.findAll(\"sp\")\n",
    "                    ],\n",
    "                }\n",
    "                for s in a.findAll(\"div\", {\"type\": \"scene\"})\n",
    "            ],\n",
    "        }\n",
    "        for a in text.findAll(\"div\", {\"type\": \"acte\"})\n",
    "    ]\n",
    "\n",
    "    finalxml = xmltodict.unparse(\n",
    "        {\n",
    "            \"TEI\": {\n",
    "                \"@xmlns\": \"http://www.tei-c.org/ns/1.0\",\n",
    "                \"teiHeader\": {\n",
    "                    \"fileDesc\": {\n",
    "                        \"titleStmt\": {\n",
    "                            \"title\": meta[\"Titre\"],\n",
    "\n",
    "                        },\n",
    "                        \"editionStmt\": {\n",
    "                            \"respStmt\": [\n",
    "                                {\n",
    "                                    \"resp\": e[\"Role\"],\n",
    "                                    \"name\": e[\"Name\"],\n",
    "                                }\n",
    "                                for e in meta[\"Responsables\"]\n",
    "                            ],\n",
    "                        },\n",
    "                        \"publicationStmt\": {\n",
    "                            \"publisher\": meta[\"Publication\"],\n",
    "                            \"date\": meta[\"Date\"],\n",
    "                        },\n",
    "                        \"sourceDesc\": {\"p\": meta[\"Source\"]},\n",
    "                    }\n",
    "                },\n",
    "                \"text\": {\n",
    "                    \"body\": {\n",
    "                        \"div\": [\n",
    "                            {\n",
    "                                \"@type\": \"acte\",\n",
    "                                \"@n\": a[\"Numéro\"],\n",
    "                                \"head\": a[\"Titre\"],\n",
    "                                \"stage\": a[\"Stage\"],\n",
    "                                \"div\": [\n",
    "                                    {\n",
    "                                        \"@type\": \"scene\",\n",
    "                                        \"@n\": s[\"Numéro\"],\n",
    "                                        \"head\": s[\"Titre\"],\n",
    "                                        \"stage\": s[\"Scène\"],\n",
    "                                        \"sp\": [\n",
    "                                            {\n",
    "                                                \"@who\": p[\"Personnage\"][\"Id\"],\n",
    "                                                \"speaker\": p[\"Personnage\"][\"Display\"],\n",
    "                                                \"p\": p[\"Réplique\"],\n",
    "                                            }\n",
    "                                            for p in s[\"Paroles\"]\n",
    "                                        ],\n",
    "                                    }\n",
    "                                    for s in a[\"Scènes\"]\n",
    "                                ],\n",
    "                            }\n",
    "                            for a in actes\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        pretty=True,\n",
    "    )\n",
    "\n",
    "    with open(output_folder / piece.name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(finalxml)\n",
    "\n",
    "    meta[\"Fichier\"] = piece.name\n",
    "    metas.append(meta)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T13:38:42.647629772Z",
     "start_time": "2023-08-30T13:38:42.642433858Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "actes = text.findAll(\"div\", {\"type\": \"acte\"})\n",
    "actes[0]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "meta_to_df = pd.DataFrame(metas)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T13:38:42.647980647Z",
     "start_time": "2023-08-30T13:38:42.642829213Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "meta_to_df.to_csv(\"metas.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T13:38:42.648168677Z",
     "start_time": "2023-08-30T13:38:42.643050438Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "\n",
    "\n",
    "def getReplique(p):\n",
    "    if p.find(\"p\"):\n",
    "        texte = p.find(\"p\").text\n",
    "    elif p.find(\"l\"):\n",
    "        texte = p.find(\"l\").text\n",
    "    else:\n",
    "        texte = p.text\n",
    "\n",
    "    return re.sub(r\"(\\s)\\1+\", r\"\\g<1>\", texte).strip()\n",
    "\n",
    "original_folder = Path(\"textes Codif\")\n",
    "output_folder = Path(\"forTXM\")\n",
    "output_folder.mkdir(exist_ok=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T13:38:42.648348163Z",
     "start_time": "2023-08-30T13:38:42.643245257Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mStopIteration\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m test \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_folder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mglob\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m*.xml\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__next__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(test)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# with test.open(\"r\", encoding=\"utf-8\") as f:\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m#     with open(\"test.xml\", \"w\", encoding=\"utf-8\") as g:\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m#         g.write(f.read().replace(\" \", \" \"))\u001B[39;00m\n",
      "\u001B[0;31mStopIteration\u001B[0m: "
     ]
    }
   ],
   "source": [
    "test = original_folder.glob(\"*.xml\").__next__()\n",
    "print(test)\n",
    "\n",
    "# with test.open(\"r\", encoding=\"utf-8\") as f:\n",
    "#     with open(\"test.xml\", \"w\", encoding=\"utf-8\") as g:\n",
    "#         g.write(f.read().replace(\" \", \" \"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T13:38:42.789046330Z",
     "start_time": "2023-08-30T13:38:42.646918323Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with test.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    soup = BeautifulSoup(f, \"xml\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T13:38:42.791426119Z",
     "start_time": "2023-08-30T13:38:42.790536272Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "soup\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T13:38:42.792117579Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "nlp.max_length = 5000000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T13:38:42.833213949Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = soup.find(\"text\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T13:38:42.833503357Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "for e in text.findAll():\n",
    "    if e.text:\n",
    "        # print(e.name)\n",
    "        if e.name in {\"castList\", \"castItem\", \"sp\", \"speaker\"}:\n",
    "            continue\n",
    "        doc = nlp(e.text)\n",
    "        # new_text = StringIO()\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            # print(f\"{token.text = } {token.pos_ = } {token.dep_ = } {token.head.text = } {token.head.pos_ = } {token.head.dep_ = }\")\n",
    "            # 1/0\n",
    "            # token = f\" <w pos=\\\"{token.pos_}\\\" dep=\\\"{token.dep_}\\ lemma=\\\"{token.lemma_}\\\" entity=\\\"{token.ent_type_}\\\">{token.text}</w> \"\n",
    "            # new_text.write(token)\n",
    "            # token = {\n",
    "            #     \"w\": {\n",
    "            #         \"@pos\": token.pos_,\n",
    "            #         \"@dep\": token.dep_,\n",
    "            #         \"@lemma\": token.lemma_,\n",
    "            #         \"@entity\": token.ent_type_,\n",
    "            #         \"#text\": token.text,\n",
    "            #     }\n",
    "            # }\n",
    "            # new_text.append(token)\n",
    "            new_token = soup.new_tag(\"w\", pos=token.pos_, dep=token.dep_, lemma=token.lemma_, entity=token.ent_type_)\n",
    "            new_token.string = token.text\n",
    "            new_text.append(new_token)\n",
    "        # e.replace_with(BeautifulSoup(new_text, \"xml\"))\n",
    "        # e.replace_with(new_text)\n",
    "        e.clear()\n",
    "        e.extend(new_text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T13:38:42.833653304Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T13:38:42.854685804Z",
     "start_time": "2023-08-30T13:38:42.833764753Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"test/test.xml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(str(soup))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T13:38:42.833862229Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T13:38:42.833963035Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
